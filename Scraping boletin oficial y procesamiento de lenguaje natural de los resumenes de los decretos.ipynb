{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para hacer scraping de la página del Boletín oficial (búsqueda avanzada)\n",
    "#Selenium es un conjunto de herramientas de software de código abierto utilizado para la \n",
    "#automatización de pruebas funcionales en aplicaciones web. Selenium proporciona una \n",
    "#interfaz de programación de aplicaciones (API) para controlar un navegador web y simular \n",
    "#la interacción humana con una página web.\n",
    "\n",
    "#Importar librerías\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "#Lista vacía\n",
    "dfs = []\n",
    "\n",
    "# configurar el webdriver\n",
    "service = Service('./chromedriver')\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# abrir la página web\n",
    "driver.get(\"https://www.boletinoficial.gob.ar/busquedaAvanzada/primera\")\n",
    "\n",
    "#Definir temas a buscar (en el campo palabra clave). La idea es buscar por tema, sin definir tipo de norma\n",
    "temas= ['precios'\n",
    "        , 'multas'\n",
    "        , 'agio'\n",
    "        , 'especulación'\n",
    "        , 'abastecimiento'\n",
    "#        , 'inflación'\n",
    "       ]\n",
    "\n",
    "#Definir años de búsqueda\n",
    "año_inicio = 1900\n",
    "año_fin = 1976\n",
    "años = list(range(año_inicio, año_fin))\n",
    "años = list(map(str, años))\n",
    "\n",
    "# buscar elementos \n",
    "#itera sobre los temas\n",
    "for j in temas:\n",
    "    #Para cada tema, si lo encuentra, intenta iterar sobre los años\n",
    "    for i in años:\n",
    "    \n",
    "        # Cambo de búsqueda palabra clave\n",
    "        search_box1 = driver.find_element(By.ID, \"palabraClave\")\n",
    "        search_box1.send_keys(j)\n",
    "\n",
    "        # Cambo de búsqueda palabra año\n",
    "        search_box2 = driver.find_element(By.ID, \"anioNormaIP\")\n",
    "        search_box2.send_keys(i)\n",
    "     \n",
    "        try:\n",
    "            # hacer clic en el botón de búsqueda\n",
    "            search_button = WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.ID, \"btnBusquedaAvanzada\")))\n",
    "            search_button.click()\n",
    "            \n",
    "            # espera a que se carguen nuevos resultados (1 segundo)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # limpiar ambos campos de entrada\n",
    "            search_box1.clear()\n",
    "            search_box2.clear()\n",
    "           \n",
    "            \n",
    "            # hacer scroll hasta el final de la página\n",
    "            while True:\n",
    "                # obtener número actual de resultados\n",
    "                num_resultados = len(driver.find_elements(By.XPATH, \"//div[@class='list-group-item list-group-item-action flex-column align-items-start']\"))\n",
    "    \n",
    "                # hacer scroll hasta el final de la página\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "                # esperar a que se carguen nuevos resultados (2 segundos)\n",
    "                time.sleep(2)\n",
    "    \n",
    "                # obtener nuevo número de resultados\n",
    "                num_nuevos_resultados = len(driver.find_elements(By.XPATH, \"//div[@class='list-group-item list-group-item-action flex-column align-items-start']\"))\n",
    "    \n",
    "                # salir del bucle si no hay más resultados nuevos\n",
    "                if num_nuevos_resultados == num_resultados:\n",
    "                    break\n",
    "\n",
    "            # extraer resultados visibles\n",
    "            resultados = driver.find_elements(By.XPATH, \"//div[contains(@class, 'col-md-12') and @class!='row']\")\n",
    "\n",
    "            #Busca el elemento \"8\" entre los resultados, que es el que contiene toda la normativa\n",
    "            normas = resultados[8].text\n",
    "            #Separa la gran string de normas en una lista\n",
    "            lista_normas = normas.split('\\n')\n",
    "\n",
    "            #Se eliminan de la lista los elementos que indican los tipos de normas que siguen\n",
    "            for item in ['LEYES', 'DECRETOS', 'RESOLUCIONES', 'DISPOSICIONES', \n",
    "                         'RESOLUCIONES GENERALES', 'RESOLUCIONES CONJUNTAS', 'AVISOS OFICIALES']:\n",
    "                if item in lista_normas:\n",
    "                    lista_normas.remove(item)\n",
    "            \n",
    "            #Se crean 4 columnas con los 4 elementos de cada norma\n",
    "            original = lista_normas\n",
    "            n = 4\n",
    "            sublistas = []\n",
    "\n",
    "            for i in range(0, len(original), n):\n",
    "                sublistas.append(original[i:i+n])\n",
    "\n",
    "            # Crear la lista de listas\n",
    "            lista_de_listas = sublistas\n",
    "\n",
    "            # Crear el DataFrame\n",
    "            df = pd.DataFrame(data=lista_de_listas, columns=['Poder', 'Norma', 'Fecha', 'Contenido'])\n",
    "            df['tema'] = j\n",
    "\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except:\n",
    "            # si no se encuentra la palabra clave, pasar a la siguiente iteración sin generar un mensaje de error\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crea un dataframe con las normas ya divididas en 4 columnas\n",
    "dfs_concatenado = pd.concat(dfs)\n",
    "dfs_concatenado = dfs_concatenado.reset_index()\n",
    "dfs_concatenado = dfs_concatenado.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procesamiento de lenguaje natural con spacy\n",
    "#SpaCy es una biblioteca de procesamiento del lenguaje natural (NLP) \n",
    "#de código abierto para Python. SpaCy se utiliza para realizar tareas \n",
    "#de procesamiento de texto, como tokenización, análisis sintáctico, \n",
    "#etiquetado POS (partes del habla), reconocimiento de entidades nombradas, \n",
    "#desambiguación de sentidos, entre otras.\n",
    "\n",
    "#SpaCy también cuenta con modelos pre-entrenados para varios idiomas, \n",
    "#incluidos inglés, español, alemán, francés, portugués, italiano y holandés, \n",
    "#lo que permite a los usuarios realizar tareas de NLP sin necesidad de entrenar modelos desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "from unidecode import unidecode\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo de lenguaje en español\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir función para analizar texto\n",
    "def analizar_texto(texto):\n",
    "    doc = nlp(texto)\n",
    "    #usar el modelo para extraer diversas entidades del resumen del contenido de cada norma \n",
    "    #Obtener sustantivos y nombres propios, verbos y adjetivos\n",
    "    sustantivos_y_nombres_propios = [token.lemma_ for token in doc if token.pos_ in ['NOUN', 'PROPN']]\n",
    "    verbos = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    adjetivos = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "    # Encontrar caracteres numéricos \n",
    "    #(que normalmente representan otras normas a las que hace referencia la que estamos analizando)\n",
    "    numeros = [token.text for token in doc if token.is_digit]\n",
    "    return sustantivos_y_nombres_propios, verbos, adjetivos, numeros\n",
    "\n",
    "#crear 4 columnas nuevas con las distintas entidades aplicando la función a la columna 'contenido' \n",
    "dfs_concatenado['Contenido'].fillna('', inplace=True)\n",
    "dfs_concatenado[['sustantivos_y_nombres_propios', 'verbos', 'adjetivos', 'numeros']] = pd.DataFrame(dfs_concatenado['Contenido'].apply(analizar_texto).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora viene mucho trabajo manua, tratando de identificar sustantivos, vervos y adjetivos de interés manualmente. Además de corregir algunas palabras mal categorizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear una lista de sustantivos para ver (a mano) cuáles de ellos referencian productos y otras entidades de interés\n",
    "# La columna de sustantivos contine listas de aquellos que aparecen en cada decreto\n",
    "lista_sustantivos = dfs_concatenado['sustantivos_y_nombres_propios'].explode()\n",
    "\n",
    "# Convertir la lista en un conjunto para eliminar duplicados\n",
    "conjunto_sustantivos = set(lista_sustantivos)\n",
    "\n",
    "# Convertir el conjunto de nuevo en una lista\n",
    "lista_final = list(conjunto_sustantivos)\n",
    "\n",
    "# Crear un nuevo DataFrame con la lista de sustantivos únicos\n",
    "df_sust = pd.DataFrame({'sustantivos': lista_final})\n",
    "\n",
    "#Este dataframe fue originalmente exportado a excel y categorizado \n",
    "#(cada búsqueda nueva que se haga puede tomar como base lo ya categorizado)\n",
    "#llamar al excel, con los productos y otras entidades ya identificados \n",
    "sustantivos_lista = pd.read_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listado de productos.xlsx')\n",
    "\n",
    "#aver qúe sustantivos de la \"lista_final\" no están entre los que ya hemos categorizado\n",
    "#Para eso primero se convierte la columna del excel en una lista\n",
    "sustantivos_ya_categorizados = sustantivos_lista['sustantivos'].tolist()\n",
    "#Luego se crea otra lista con aquellos de \"lista_final\" que no están entre los ya categorizados\n",
    "no_estan = [sust for sust in lista_final if sust not in sustantivos_ya_categorizados]\n",
    "#Por último, se crea un dataframe con la lista de los que no están, agregando columnas \n",
    "#con categorías vacías para categorizarlos a mano\n",
    "df_sust_no_estan = pd.DataFrame({'sustantivos': no_estan, 'PRODUCTO': \"\", 'agio-espec': \"\", 'precio': \"\", 'multa': \"\", 'otra norma': \"\", 'max_bas': \"\", 'acciones': \"\", 'licitación-concurso-adqui': \"\", 'abastecimiento': \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sumar los nuevos sustantivos\n",
    "df_sust_nuevo = pd.concat([sustantivos_lista, df_sust_no_estan])\n",
    "df_sust_nuevo = df_sust_nuevo.reset_index()\n",
    "df_sust_nuevo = df_sust_nuevo.drop('index', axis=1)\n",
    "\n",
    "#exportar a excel para proceder a categorizar\n",
    "df_sust_nuevo.to_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listado de productos.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acá viene un paso intermedio manual, que es el de identificar productos, referencias a agio y especulación, precios, etc. La mayoría están identificados, pero pueden aparecer más dependiendo de los rangos de fechas y palabras claves buscadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamar al excel de nuevo, con los productos agregados en el paso anterior ya identificados\n",
    "sustantivos_lista = pd.read_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listado de productos.xlsx')\n",
    "\n",
    "#Armar listas con los sustantivos que encajan en las categorías definidas\n",
    "productos = sustantivos_lista[sustantivos_lista['PRODUCTO'] == 1]\n",
    "sustantivos_excluidos = productos['sustantivos'].tolist()\n",
    "\n",
    "precio = sustantivos_lista[sustantivos_lista['precio'] == 1]\n",
    "sustantivos_excluidos_precio = precio['sustantivos'].tolist()\n",
    "\n",
    "agio_espec = sustantivos_lista[sustantivos_lista['agio-espec'] == 1]\n",
    "sustantivos_excluidos_agio = agio_espec['sustantivos'].tolist()\n",
    "\n",
    "multas = sustantivos_lista[sustantivos_lista['multa'] == 1]\n",
    "sustantivos_excluidos_multas = multas['sustantivos'].tolist()\n",
    "\n",
    "normas = sustantivos_lista[sustantivos_lista['otra norma'] == 1]\n",
    "sustantivos_excluidos_normas = normas['sustantivos'].tolist()\n",
    "\n",
    "licit_concur_adqui = sustantivos_lista[sustantivos_lista['licitación-concurso-adqui'] == 1]\n",
    "sustantivos_excluidos_licit = licit_concur_adqui['sustantivos'].tolist()\n",
    "\n",
    "abastecimiento = sustantivos_lista[sustantivos_lista['abastecimiento'] == 1]\n",
    "sustantivos_excluidos_abast = abastecimiento['sustantivos'].tolist()\n",
    "\n",
    "#adjetivos en sustantivos\n",
    "max_bas = sustantivos_lista[sustantivos_lista['max_bas'] == 1]\n",
    "adj_excluidos1 = max_bas['sustantivos'].tolist()\n",
    "\n",
    "#verbos en sustantivos\n",
    "acción = sustantivos_lista[sustantivos_lista['acciones'] == 1]\n",
    "verb_excluidos1 = acción['sustantivos'].tolist()\n",
    "\n",
    "todos_los_excluidos = sustantivos_excluidos+sustantivos_excluidos_precio+sustantivos_excluidos_agio+sustantivos_excluidos_multas+sustantivos_excluidos_normas+adj_excluidos1+verb_excluidos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir la columna sustantivos en cada categoría y en otros sustantivos\n",
    "# Crear una lista de sustantivos que están en la lista de excluidos\n",
    "sustantivos_producto = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in sustantivos_excluidos]))\n",
    "sustantivos_precio = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in sustantivos_excluidos_precio]))\n",
    "sustantivos_agio = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in sustantivos_excluidos_agio]))\n",
    "sustantivos_multa = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in sustantivos_excluidos_multas]))\n",
    "sustantivos_normas = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in sustantivos_excluidos_normas]))\n",
    "sustantivos_max_bas = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in adj_excluidos1]))\n",
    "sustantivos_accion = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in verb_excluidos1]))\n",
    "sustantivos_licit = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in sustantivos_excluidos_licit]))\n",
    "sustantivos_abast = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo in sustantivos_excluidos_abast]))\n",
    "\n",
    "todos_los_excluidos = sustantivos_excluidos+sustantivos_excluidos_precio+sustantivos_excluidos_agio+sustantivos_excluidos_multas+sustantivos_excluidos_normas+adj_excluidos1+verb_excluidos1+sustantivos_excluidos_licit+sustantivos_excluidos_abast\n",
    "\n",
    "# crear una lista de sustantivos que no están en la lista de excluidos\n",
    "sustantivos_no_ProdPreAgioMult = list(set([sustantivo for sustantivos in dfs_concatenado['sustantivos_y_nombres_propios'] for sustantivo in sustantivos if sustantivo not in todos_los_excluidos]))\n",
    "\n",
    "# crear nuevas columnas con las listas de sustantivos específicos y los otros\n",
    "dfs_concatenado['productos'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(sustantivos_excluidos)))\n",
    "dfs_concatenado['precio'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(sustantivos_excluidos_precio)))\n",
    "dfs_concatenado['agio'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(sustantivos_excluidos_agio)))\n",
    "dfs_concatenado['multa'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(sustantivos_excluidos_multas)))\n",
    "dfs_concatenado['norma'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(sustantivos_excluidos_normas)))\n",
    "dfs_concatenado['licit_concur_adqui 1'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(sustantivos_excluidos_licit)))\n",
    "dfs_concatenado['abastecimiento'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(sustantivos_excluidos_abast)))\n",
    "dfs_concatenado['adj max bas 1'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(adj_excluidos1)))\n",
    "dfs_concatenado['acciones 1'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) & set(verb_excluidos1)))\n",
    "dfs_concatenado['otros sustantivos'] = dfs_concatenado['sustantivos_y_nombres_propios'].apply(lambda sustantivos: list(set(sustantivos) - set(todos_los_excluidos)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERBOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacemos algo similar con los verbos\n",
    "lista_verbos = dfs_concatenado['verbos'].explode()\n",
    "\n",
    "conjunto_verbos = set(lista_verbos)\n",
    "\n",
    "\n",
    "lista_final = list(conjunto_verbos)\n",
    "\n",
    "\n",
    "df_verbos = pd.DataFrame({'verbos': lista_final})\n",
    "\n",
    "\n",
    "df_verbos.to_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\verbos_unicos.xlsx', index=False)\n",
    "\n",
    "\n",
    "acciones_lista = pd.read_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listado de acciones.xlsx')\n",
    "\n",
    "\n",
    "verbos_ya_categorizados = acciones_lista['verbos'].tolist()\n",
    "\n",
    "no_estan = [verb for verb in lista_final if verb not in verbos_ya_categorizados]\n",
    "\n",
    "df_verb_no_estan = pd.DataFrame({'verbos': no_estan, 'ACCION': \"\",  'max-bas': \"\", 'licitación-concurso-adqui': \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_verb_nuevo = pd.concat([acciones_lista, df_verb_no_estan])\n",
    "df_verb_nuevo = df_verb_nuevo.reset_index()\n",
    "df_verb_nuevo = df_verb_nuevo.drop('index', axis=1)\n",
    "\n",
    "\n",
    "df_verb_nuevo.to_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listado de acciones.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acá viene un paso intermedio manual, que es el de identificar acciones de interés. La mayoría están identificados, pero pueden aparecer más dependiendo de los rangos de fechas y palabras claves buscadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acciones_lista = pd.read_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listado de acciones.xlsx')\n",
    "\n",
    "acciones = acciones_lista[acciones_lista['ACCION'] == 1]\n",
    "verbos_excluidos = acciones['verbos'].tolist()\n",
    "\n",
    "licit_concur_adqui = acciones_lista[acciones_lista['licitación-concurso-adqui'] == 1]\n",
    "verb_excluidos_licit = licit_concur_adqui['verbos'].tolist()\n",
    "\n",
    "\n",
    "max_bas = acciones_lista[acciones_lista['max-bas'] == 1]\n",
    "adj_excluidos2 = max_bas['verbos'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "verbos_acción = list(set([verbo for verbo in dfs_concatenado['verbos'] for verbo in verbo if verbo in verbos_excluidos]))\n",
    "adj_lista_verb = list(set([adj for adj in dfs_concatenado['verbos'] for adj in adj if adj in adj_excluidos1]))\n",
    "licit_cocnur_adqui = list(set([verbo for verbo in dfs_concatenado['verbos'] for verbo in verbo if verbo in verb_excluidos_licit]))\n",
    "\n",
    "todos_los_verb_excluidos = verbos_excluidos+adj_excluidos1+verb_excluidos_licit\n",
    "\n",
    "\n",
    "verbos_no_acción = list(set([verbo for verbo in dfs_concatenado['verbos'] for verbo in verbo if verbo not in verbos_excluidos]))\n",
    "\n",
    "\n",
    "dfs_concatenado['acciones 2'] = dfs_concatenado['verbos'].apply(lambda verbos: list(set(verbos) & set(verbos_excluidos)))\n",
    "dfs_concatenado['licit_concur_adqui 2'] = dfs_concatenado['verbos'].apply(lambda verbos: list(set(verbos) & set(verb_excluidos_licit)))\n",
    "dfs_concatenado['adj max bas 2'] = dfs_concatenado['verbos'].apply(lambda verbos: list(set(verbos) & set(adj_excluidos2)))\n",
    "dfs_concatenado['otros verbos'] = dfs_concatenado['verbos'].apply(lambda verbos: list(set(verbos) - set(todos_los_verb_excluidos)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unir_listas(row, cols):\n",
    "    lista_unida = []\n",
    "    for col in cols:\n",
    "        lista_unida.extend(row[col])\n",
    "    return lista_unida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalmente podemos crear una columna que contenga referencias a licitaciones\n",
    "dfs_concatenado['licit_concur_adqui'] = dfs_concatenado.apply(unir_listas, axis=1, args=(['licit_concur_adqui 1', 'licit_concur_adqui 2'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalmente podemos crear una columna que contenga las acciones\n",
    "dfs_concatenado['acciones'] = dfs_concatenado.apply(unir_listas, axis=1, args=(['acciones 1', 'acciones 2'],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adjetivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_adj = dfs_concatenado['adjetivos'].explode()\n",
    "\n",
    "\n",
    "conjunto_adj = set(lista_adj)\n",
    "\n",
    "\n",
    "lista_final = list(conjunto_adj)\n",
    "\n",
    "\n",
    "df_adj = pd.DataFrame({'adjetivos': lista_final})\n",
    "\n",
    "\n",
    "df_adj.to_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\adjetivos_unicos.xlsx', index=False)\n",
    "\n",
    "\n",
    "adj_lista = pd.read_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listados max_bas.xlsx')\n",
    "\n",
    "\n",
    "adj_ya_categorizados = adj_lista['adjetivos'].tolist()\n",
    "\n",
    "no_estan = [adj for adj in lista_final if adj not in adj_ya_categorizados]\n",
    "\n",
    "df_adj_no_estan = pd.DataFrame({'adjetivos': no_estan, 'max_bas': \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_adj_nuevo = pd.concat([adj_lista, df_adj_no_estan])\n",
    "df_adj_nuevo = df_adj_nuevo.reset_index()\n",
    "df_adj_nuevo = df_adj_nuevo.drop('index', axis=1)\n",
    "\n",
    "\n",
    "df_adj_nuevo.to_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listados max_bas.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acá viene un paso intermedio manual, que es el de identificar maximos, minimos, básicos, etc. La mayoría están identificados, pero pueden aparecer más dependiendo de los rangos de fechas y palabras claves buscadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adj_lista = pd.read_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\listados max_bas.xlsx')\n",
    "max_bas = adj_lista[adj_lista['max_bas'] == 1]\n",
    "adj_excluidos3 = max_bas['adjetivos'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_lista = list(set([adj for adj in dfs_concatenado['adjetivos'] for adj in adj if adj in adj_excluidos3]))\n",
    "\n",
    "# Creamos una lista de verbos que no están en la lista de excluidos\n",
    "adj_no_lista = list(set([adj for adj in dfs_concatenado['adjetivos'] for adj in adj if adj not in adj_excluidos3]))\n",
    "\n",
    "# Creamos dos nuevas columnas con las listas de verbos\n",
    "dfs_concatenado['adj max bas 3'] = dfs_concatenado['adjetivos'].apply(lambda verbos: list(set(verbos) & set(adj_excluidos3)))\n",
    "dfs_concatenado['otros adjetivos'] = dfs_concatenado['adjetivos'].apply(lambda verbos: list(set(verbos) - set(adj_excluidos3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalmente podemos crear una columna que contenga el tipo de adjetivos de interés\n",
    "dfs_concatenado['adj max bas'] = dfs_concatenado.apply(unir_listas, axis=1, args=(['adj max bas 1', 'adj max bas 2', 'adj max bas 3'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mejorar algunas columnas\n",
    "dfs_concatenado[['Norma', 'Año']] = dfs_concatenado['Norma'].str.split('/', n=1, expand=True)\n",
    "dfs_concatenado[['Norma', 'Número']] = dfs_concatenado['Norma'].str.split(' ', n=1, expand=True)\n",
    "dfs_concatenado['Fecha'] = dfs_concatenado['Fecha'].str.replace('Fecha de Publicacion: ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso manual para corregir la escritura d elas acciones de interés\n",
    "acciones_originales = dfs_concatenado['acciones'].explode()\n",
    "acciones_originales = [valor for valor in acciones_originales if valor == valor]\n",
    "\n",
    "acciones_originales = sorted(list(set(acciones_originales)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-modificación', '11,747.fijanse', 'AJUSTAN', 'AJUSTE', 'AUTORIZACION', 'AUTORIZAR', 'AUTORIZASE', 'Ajuste', 'Autoriz', 'Autoriza', 'Autorización', 'Autorizando', 'Autorizanse', 'Autorízase', 'Contabilidad.-Reajuste', 'DEROGASE', 'Deroga', 'Derogación', 'Deroganse', 'Deróganse', 'Derógase', 'Déjanse', 'Déjase', 'EFECTO', 'ESTABLECEN', 'ESTABLECENSE', 'ESTABLECER', 'ESTABLECIDO', 'ESTABLECIDOS', 'Efecto', 'Establecese', 'FIJA', 'FIJADAS', 'FIJADOS', 'FIJAN', 'FIJANSE', 'FIJARAN', 'FIJASE', 'Fija', 'Fijación', 'Fijan', 'Fijando', 'Fijanse', 'Fijase', 'Fíajanse', 'Fíja', 'Fíjanse', 'Fíjase', 'MODIFICACION', 'MODIFICAN', 'MODIFICANSE', 'MODIFICAR', 'Modif', 'Modific', 'Modifica', 'Modificacion', 'Modificación', 'Modificanse', 'Modificase', 'Modifícanse', 'Modifícase', 'REAJUSTASE', 'REAJUSTE', 'Reajusta', 'Reajustanse', 'Reajuste', 'Reajustes', 'Reajústanse', 'Rebaja', 'Rebajas', 'actualizanse', 'actualizar', 'aestableceir', 'ajustar', 'ajustar él', 'ajuste', 'amplianse', 'ampliar', 'autorización', 'autorizan', 'autorizase', 'dejansir', 'dejar', 'derogación', 'deroganir', 'derogansir', 'derogar', 'derógansir', 'determinar', 'determinarir', 'efecto', 'eliminiación', 'elimínansar', 'elévanse', 'establece', 'establecen', 'establecer', 'establecido', 'estableciendose', 'establez', 'establezca', 'exceptúase', 'excluer', 'excluir', 'exclusion', 'excluyendoir', 'ffijar', 'fija él', 'fijación', 'fijado', 'fijan', 'fijar', 'fijar él', 'fijir', 'fijándosir', 'flexibilización', 'fíjanse', 'fíjar', 'incluer', 'incluir', 'incluir él', 'inclúir', 'incorporar', 'incrementar', 'liberación', 'liberanse', 'liberar', 'modicicar', 'modificacion', 'modificación', 'modifican', 'modificar', 'modificatorio', 'modificir', 'modifícanse', 'modifícar', 'modifícase', 'reajustar', 'reajuste', 'rebaja', 'rebajar', 'suprimir']\n"
     ]
    }
   ],
   "source": [
    "print(acciones_originales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "acciones_corregidas = ['modificar', 'fijar', 'ajustar', 'ajustar', 'autorizar', 'autorizar', 'autorizar', \n",
    "                       'ajustar', 'autorizar', 'autorizar', 'autorizar', 'autorizar', 'autorizar', 'autorizar', \n",
    "                       'reajustar', 'derogar', 'derogar', 'derogar', 'derogar', 'derogar', 'derogar', \n",
    "                       'dejar', 'dejar', 'efecto', 'establecer', 'establecer', 'establecer', 'establecer', \n",
    "                       'establecer', 'efecto', 'establecer', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', \n",
    "                       'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', \n",
    "                       'fijar', 'modificar', 'modificar', 'modificar', 'modificar', 'modificar', 'modificar', 'modificar', \n",
    "                       'modificar', 'modificar', 'modificar', 'modificar', 'modificar', 'modificar', 'reajustar', \n",
    "                       'reajustar', 'reajustar', 'reajustar', 'reajustar', 'reajustar', 'reajustar', 'rebajar', 'rebajar', \n",
    "                       'actualizar', 'actualizar', 'establecer', 'ajustar', 'ajustar', 'ajustar', 'ampliar', \n",
    "                       'ampliar', 'autorizar', 'autorizar', 'autorizar', 'dejar', 'dejar', 'derogar', 'derogar', \n",
    "                       'derogar', 'derogar', 'derogar', 'determinar', 'determinar', 'efecto', 'eliminar', \n",
    "                       'eliminar', 'elevar', 'establecer', 'establecer', 'establecer', 'establecer', 'establecer', \n",
    "                       'establecer', 'establecer', 'establecer', 'excluir', 'excluir', 'excluir', 'excluir', 'fijar', \n",
    "                       'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'fijar', 'flexibilizar', \n",
    "                       'fijar', 'fijar', 'incluir', 'incluir', 'incluir', 'incluir', 'incorporar', 'incrementar', \n",
    "                       'liberar', 'liberar', 'liberar', 'modificar', 'modificar', 'modificar', 'modificar', \n",
    "                       'modificar', 'modificar', 'modificar', 'modificar', 'modificar', 'modificar', 'reajustar', \n",
    "                       'reajustar', 'rebajar', 'rebajar', 'suprimir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acciones_corregidas = pd.DataFrame({'acciones de interés': acciones_originales, 'acciones_corregidas': acciones_corregidas})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_concatenado['acciones corregidas'] = dfs_concatenado['acciones']\n",
    "\n",
    "# Crea el diccionario de reemplazo\n",
    "reemplazo = dict(zip(acciones_corregidas['acciones de interés'], acciones_corregidas['acciones_corregidas']))\n",
    "\n",
    "# Aplica la función de reemplazo en cada celda del DataFrame\n",
    "dfs_concatenado['acciones corregidas'] = dfs_concatenado['acciones corregidas'].apply(lambda x: [reemplazo.get(i, i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_concatenado[\"Año\"] = dfs_concatenado[\"Año\"].str.replace(\"N/\", \"\")\n",
    "dfs_concatenado[\"Año\"] = dfs_concatenado[\"Año\"].str.replace(\"n/\", \"\")\n",
    "\n",
    "dfs_concatenado['Año'] = pd.to_numeric(dfs_concatenado['Año'] , errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos nuestro dataset con información categorizada de decretos de diversos temas y \n",
    "#con la amplitud de fechas que queramos\n",
    "\n",
    "dfs_concatenado.to_excel(r'C:\\Users\\lucat\\OneDrive\\HEAnuevo\\decretos 1900-1975.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
